{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This blogposts elaborates on how the data was processed for the DengAI data science challenge on DrivenData.org.\n",
    "\n",
    "## Motivation for Pipelines\n",
    "When it comes to feature engineering, or any kind of data processing the problem quickly arises that we have to carry the data from one processing step to the next. This act is not only tedious, but also prawn to errors. Consider the following example where we would like to impute the missing observations first with a KNN Imputation and afterwards scale the data. In the traditional approach one would have to assign the results from the first step (imputation) and feed the output from the first step into the second step, as shown in the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0. ]\n",
      " [0.5 0.  1. ]\n",
      " [1.  1.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data = np.array([[1, np.nan, 3], [4, 5, 6], [7, 8, np.nan]])\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "scaled_data = MinMaxScaler().fit_transform(imputed_data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above the idea it becomes apparent, why this method is prunet to errors. Always assigning the output from the last step as the input of the following step also seems unnecessary tedious. One workaround for that would be to use pipelines. A *pipeline* is a concept from scikit-learn in which all steps are aligned and executed one after another. For the example above, this would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0. ],\n",
       "       [0.5, 0. , 1. ],\n",
       "       [1. , 1. , 0.5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    KNNImputer(),\n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workings of pipelines do not only have much nicer syntax, they also are especially useful when are transforming not only one series, but multiple. Furthermore, this method is particularly useful within prediction tasks. This is because of the nature of training and test data.\n",
    "\n",
    "A popular beginner mistake when doing a forecasting challenge is *data leakgage*. Data leakage describes the state in which any kind of information within the training data is used in the test data. When, for example, we mean encode a variable using the entire dataset and conduct the train-test-split afterwards, then the mean-encoded column within the test data contains information from the trainings data. Data leakage is problematic, given that is biases the prediction result.\n",
    "\n",
    "Trying to mitigate data leakge without using pipelines is quite tedious work, which the following code snippet shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.33)\n",
    "\n",
    "# Fitting everything on X\n",
    "imputer.fit(X_train)\n",
    "imputed_data = imputer.transform(transform)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "\n",
    "# Transforming on Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dengue_ensemble",
   "language": "python",
   "name": "dengue_ensemble"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
